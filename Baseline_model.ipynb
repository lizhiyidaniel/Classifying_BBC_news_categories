{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d35c4549-ae6b-40c0-b952-a87f15262ef7",
   "metadata": {},
   "source": [
    "1. Reads in the BBC topic dataset \n",
    "2. Represents its text as vectors using the TFIDF vectorizer\n",
    "3. Represents the topic labels as numbers\n",
    "4. Analyzes the number of texts available for each topic and the most frequent words for the 5. two most numerous topics\n",
    "6. Trains a dummy classifier on the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77956d4-100b-4678-a994-99aa74700ec4",
   "metadata": {},
   "source": [
    "### Import the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c75ab4a3-73bd-4d7b-a376-749f31a1b6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.probability import FreqDist\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn import preprocessing\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import math\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a36e21c-6a71-4214-8a60-bbffc26b0fff",
   "metadata": {},
   "source": [
    "### Initialize global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "922f5079-8d31-47cc-820e-75fa0007b3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.geeksforgeeks.org/snowball-stemmer-nlp/\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fc84bda-61eb-4117-843a-2a9171d4c47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"bbc-text.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a976fe2d-d8ab-4aab-879a-810bebb0858f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c9e89f-6163-4661-83c7-acc1fc7265c6",
   "metadata": {},
   "source": [
    "### read the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e77ecbb-1f47-4501-b566-af406cb6c73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_in_csv(csv_file):\n",
    "    with open(csv_file, 'r', encoding='utf-8') as fp:\n",
    "        reader = csv.reader(fp, delimiter=',',\n",
    "                            quotechar='\"')\n",
    "        data_read = [row for row in reader]\n",
    "    return data_read"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dad469-de3e-4494-9b75-c6c3bc9de332",
   "metadata": {},
   "source": [
    "### tokenize and stem words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d31976d-a38e-4860-a713-9571c0955780",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_stem(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    filtered_tokens = [t for t in tokens if t not in\n",
    "                       string.punctuation]\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd72d2ce-0d0f-42b5-afa9-6e2661c89f71",
   "metadata": {},
   "source": [
    "### stemmed stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86747206-feaf-485e-8f2d-8b9168052974",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [stemmer.stem(word) for word in stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0e4352-6f6b-4115-a8fe-cb9473de654f",
   "metadata": {},
   "source": [
    "### represent the input data as dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7db15df8-e720-4287-a37b-eebb5e99a030",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(filename):\n",
    "    data = read_in_csv(filename)\n",
    "    data_dict = {}\n",
    "    for row in data[1:]:\n",
    "        category = row[0]\n",
    "        text = row[1]\n",
    "        if (category not in data_dict.keys()):\n",
    "            data_dict[category] = []\n",
    "        data_dict[category].append(text)\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bd9f14a-3d13-43ee-91e3-59b233135ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = get_data(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a582616-1084-4ed8-9a1d-dd5db27a0044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tech \t 401\n",
      "business \t 510\n",
      "sport \t 511\n",
      "entertainment \t 386\n",
      "politics \t 417\n"
     ]
    }
   ],
   "source": [
    "for topic in data_dict.keys():\n",
    "    print(topic, \"\\t\", len(data_dict[topic]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4ac688-7828-46af-95e0-9a4b3a581be3",
   "metadata": {},
   "source": [
    "### get stats and use FreqDist to provide the top words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ec50229-c033-4273-9878-6d7cef5a5e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(text, num_words=200):\n",
    "    word_list = nltk.tokenize.word_tokenize(text)\n",
    "    word_list = [word for word in word_list if word\n",
    "                 not in stopwords and re.search(\n",
    "                                    \"[A-Za-z]\", word)]\n",
    "    freq_dist = FreqDist(word_list)\n",
    "    print(freq_dist.most_common(num_words))\n",
    "    return freq_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cac58fd3-57ca-4a25-8e68-4be493b1e731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('said', 1680), ('its', 1100), ('us', 813), ('year', 637), ('mr', 600), ('would', 463), ('also', 440), ('market', 425), ('new', 416), ('company', 415), ('growth', 384), ('last', 365), ('firm', 362), ('economy', 359), ('government', 340), ('bank', 335), ('sales', 316), ('could', 311), ('economic', 310), ('oil', 294), ('shares', 265), ('however', 256), ('world', 252), ('may', 251), ('years', 247), ('prices', 246), ('one', 243), ('chief', 236), ('two', 231), ('china', 223), ('business', 218), ('companies', 212), ('analysts', 209), ('uk', 207), ('deal', 206), ('rise', 203), ('expected', 200), ('group', 199), ('financial', 197), ('yukos', 196), ('firms', 193), ('since', 183), ('dollar', 180), ('december', 173), ('country', 173), ('months', 170), ('people', 170), ('stock', 168), ('first', 165), ('president', 165), ('three', 164), ('still', 164), ('many', 163), ('time', 159), ('european', 159), ('rate', 159), ('state', 158), ('trade', 158), ('told', 155), ('investment', 153), ('demand', 151), ('interest', 151), ('india', 151), ('quarter', 149), ('figures', 149), ('profits', 148), ('any', 148), ('rates', 148), ('made', 147), ('countries', 146), ('spending', 146), ('executive', 145), ('news', 143), ('biggest', 142), ('month', 141), ('strong', 139), ('price', 139), ('jobs', 137), ('europe', 136), ('next', 136), ('added', 135), ('foreign', 134), ('tax', 132), ('much', 132), ('back', 131), ('rose', 131), ('only', 130), ('offer', 130), ('euros', 130), ('budget', 129), ('according', 129), ('bid', 128), ('costs', 127), ('high', 127), ('set', 126), ('money', 126), ('exchange', 125), ('recent', 123), ('during', 122), ('january', 122), ('investors', 122), ('part', 122), ('increase', 121), ('industry', 120), ('share', 120), ('cut', 118), ('fall', 117), ('make', 116), ('million', 116), ('hit', 116), ('week', 115), ('well', 112), ('london', 112), ('russian', 111), ('move', 110), ('japan', 110), ('take', 110), ('court', 109), ('deutsche', 109), ('former', 108), ('higher', 108), ('debt', 108), ('report', 108), ('being', 107), ('says', 107), ('united', 106), ('production', 106), ('likely', 106), ('pay', 105), ('fell', 103), ('reported', 102), ('annual', 101), ('deficit', 101), ('minister', 100), ('because', 100), ('say', 100), ('consumer', 99), ('russia', 99), ('despite', 98), ('sale', 98), ('earlier', 97), ('global', 95), ('bankruptcy', 95), ('exports', 95), ('markets', 95), ('eu', 95), ('south', 94), ('international', 94), ('number', 93), ('continue', 92), ('club', 92), ('shareholders', 91), ('cost', 90), ('plans', 90), ('record', 90), ('euro', 89), ('seen', 89), ('less', 89), ('public', 89), ('main', 89), ('giant', 88), ('november', 88), ('profit', 88), ('unit', 87), ('trading', 86), ('future', 86), ('end', 86), ('case', 86), ('largest', 85), ('car', 85), ('before', 85), ('fraud', 84), ('statement', 84), ('boost', 84), ('need', 83), ('put', 83), ('sector', 83), ('work', 83), ('already', 82), ('lost', 82), ('agreed', 82), ('meeting', 82), ('takeover', 81), ('german', 81), ('buy', 81), ('gm', 80), ('finance', 80), ('low', 79), ('stake', 79), ('lse', 79), ('decision', 78), ('previous', 78), ('domestic', 78), ('value', 78), ('banks', 78), ('warned', 77), ('see', 77), ('including', 77), ('came', 77), ('saying', 77)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FreqDist({'said': 1680, 'its': 1100, 'us': 813, 'year': 637, 'mr': 600, 'would': 463, 'also': 440, 'market': 425, 'new': 416, 'company': 415, ...})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "business_data = data_dict[\"business\"]\n",
    "business_string = \" \".join(business_data)\n",
    "get_stats(business_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7e41d4-d164-4b5c-9864-cfdd9ab5f038",
   "metadata": {},
   "source": [
    "### create TFIDF vectorizer for the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55fb1604-f7f5-4745-b34c-64579b312813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vectorizer(text_list):\n",
    "    tfidf_vectorizer = \\\n",
    "    TfidfVectorizer(max_df=0.90, max_features=200000,\n",
    "                    min_df=0.05, stop_words='english',\n",
    "                    use_idf=True,\n",
    "                    tokenizer=tokenize_and_stem,\n",
    "                    ngram_range=(1,3))\n",
    "    tfidf_vectorizer.fit_transform(text_list)\n",
    "    return tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97775c5-fce9-45ed-94b3-000fd8b59cdd",
   "metadata": {},
   "source": [
    "### split the train and test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ebcd646-fec8-45c0-9be4-5935d69e5b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_test_train(data, train_percent):\n",
    "    train_test_border = \\\n",
    "    math.ceil(train_percent*len(data))\n",
    "    train_data = data[0:train_test_border]\n",
    "    test_data = data[train_test_border:]\n",
    "    return (train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8c4bd27-5289-486b-9e83-b88d5b1d18e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sports_data = data_dict[\"sport\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10eccbb3-a410-4418-b5bd-559f159df4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lizhiyi/opt/miniconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "(business_train_data, business_test_data) = \\\n",
    "split_test_train(business_data, 0.8)\n",
    "(sports_train_data, sports_test_data) = \\\n",
    "split_test_train(sports_data, 0.8)\n",
    "train_data = business_train_data + sports_train_data\n",
    "tfidf_vec = create_vectorizer(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be9e291-4209-4a6f-bc4d-9914577a39ce",
   "metadata": {},
   "source": [
    "### get labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "596dcdb3-d705-41c5-9359-fd8ace8dea3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(names):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(names)\n",
    "    return le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a550ec5a-f704-44d6-9806-a3eff728a91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = get_labels([\"business\", \"sport\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9ca39c-ef0b-468a-8434-50baa95f2d8a",
   "metadata": {},
   "source": [
    "### create dataset to transform the text data into a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c4d2894-2be0-4933-a001-838cbe7747e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(vectorizer, data_dict, le):\n",
    "    business_news = data_dict[\"business\"]\n",
    "    sports_news = data_dict[\"sport\"]\n",
    "    (sports_vectors, sports_labels) = create_data_matrix(sports_news, vectorizer, \"sport\", le)\n",
    "    (business_vectors, business_labels) = create_data_matrix(business_news, vectorizer, \"business\", le)\n",
    "    all_data_matrix = np.vstack((business_vectors, sports_vectors))\n",
    "    labels = np.concatenate([business_labels, sports_labels])\n",
    "    return (all_data_matrix, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b082d703-7d41-4593-ab87-fb96f9ec667b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_matrix(input_data, vectorizer, label, le):\n",
    "    vectors = vectorizer.transform(input_data).todense()\n",
    "    labels = [label]*len(input_data)\n",
    "    enc_labels = le.transform(labels)\n",
    "    return (vectors, enc_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaf0a40-2755-4c1d-b100-aeaefa9b9f2f",
   "metadata": {},
   "source": [
    "### create two data dictionaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7860cd17-fee6-4a5b-b5c8-97eaca4f78cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dict = {'business':business_train_data,\n",
    "                   'sport':sports_train_data}\n",
    "test_data_dict = {'business':business_test_data,\n",
    "                  'sport':sports_test_data}\n",
    "(X_train, y_train) = \\\n",
    "create_dataset(tfidf_vec, train_data_dict, le)\n",
    "(X_test, y_test) = \\\n",
    "create_dataset(tfidf_vec, test_data_dict, le)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1912e10-7537-4a7c-ab28-8fcfa2c4465f",
   "metadata": {},
   "source": [
    "### create dummy classifier to establish a baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3264915c-5e14-45dd-8708-0cedc46d5dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_trivial(X_train, y_train, X_test, y_test, le):\n",
    "    dummy_clf = DummyClassifier(strategy='uniform',\n",
    "                                random_state=0)\n",
    "    dummy_clf.fit(X_train, y_train)\n",
    "    y_pred = dummy_clf.predict(X_test)\n",
    "    print(dummy_clf.score(X_test, y_test))\n",
    "    print(classification_report(y_test, y_pred,\n",
    "          labels=le.transform(le.classes_),\n",
    "          target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a8072ed9-5673-486e-b1dd-639cc6a252d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44607843137254904\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    business       0.45      0.44      0.44       102\n",
      "       sport       0.45      0.45      0.45       102\n",
      "\n",
      "    accuracy                           0.45       204\n",
      "   macro avg       0.45      0.45      0.45       204\n",
      "weighted avg       0.45      0.45      0.45       204\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_trivial(X_train, y_train, X_test, y_test, le)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
